---
title: "MCSE-Free CRM Performance and Safety Assessment"
author: "David C. Norris"
date: "4/25/2021"
output:
  bookdown::tufte_html2:
    highlight: pygments
vignette: <
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{MCSE-Free CRM Performance and Safety}
  \usepackage[utf8]{inputenc}
bibliography:
  - precautionary-package.bib
  - packages.bib
header-includes:
  \newcommand{\MTDi}{\mathrm{MTD}_i}
  \newcommand{\MTDig}[1][g]{\mathrm{MTD}_i^{#1}}
  \newcommand{\CV}{\mathrm{CV}}

---

```{r setup, include=FALSE}
old <- options(rmarkdown.html_vignette.check_title = FALSE) # suppress un-needed warning
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.height = 4, fig.width = 6)
knitr::opts_knit$set(eval.after = "fig.cap")

library(precautionary)
library(knitr)
library(kableExtra)

options(ordinalizer = NULL) # vignette presumes this is not already set

# Echo source focused on package functionality by redacting calls to kable()
# see https://bookdown.org/yihui/rmarkdown-cookbook/hook-hide.html:
local({
  hook_source <- knitr::knit_hooks$get('source')
  knitr::knit_hooks$set(source = function(x, options) {
    x <- gsub(" -> etc.", "", x)
    x <- x[!grepl("^etc. %>%", x)] # strip lines starting "etc. %>% ..."
    x <- x[!grepl("\\(etc.,", x)]  # strip lines with fun(etc., ...)
    hook_source(x, options)
  })
})
```

> "In most trials, and in particular in cancer chemotherapy trials, it is rare that we have no idea at all about the dose-response relationship. Such information is implicitly used in the choice of dose levels available for use in the trial, and part of our approach here is to make an attempt to quantify such information."^[@oquigley_continual_1990]

> "It is important to see to what extent the largely arbitrary specification of these features influences the operational nature of the method, as well as the conclusions reached.
> Experimenters prefer methods which are relatively insensitive to prior specification as well as the form of the working model."^[@chevret_continual_1993]

> "Most clinicians do not feel sufficiently confident in their initial toxicity probability estimates to start above the lowest dose (often chosen to be 10% of rodents' LD$_{10}$)."^[@goodman_practical_1995]

> "The main challenge when using the CRM is model calibration."^[@lee_calibration_2011]


# Background

Since its inception 3 decades ago [@oquigley_continual_1990], the CRM has retreated fully from the sincere Bayesianism of its original claim to manifest clinically relevant prior information. The early retreat was quite swift, in fact. Section 2.4 of [@oquigley_continual_1990], titled 'Establishing Dose Levels', suggested a substantal collaboration between modelers and clinical investigators. Yet already by 1993, [@chevret_continual_1993] accepted "that $k$ distinct doses are chosen for experimentation, defined by the investigator through an implicit idea of the dose-toxicity relationship, however imprecise," and demoted the CRM's prior dose-toxicity curve to the status of a (multidimensional) *tuning parameter*. The retreat continued apace with @goodman_practical_1995 giving voice to the deep skepticism with which clinical investigators were already viewing a CRM that failed to capture important intutions. Indeed, a key "practical improvement" for which that paper is still cited, is its abandonment of any role for the  prior curve in informing the trial starting dose.

By twenty years later, biostatisticians' had completed their retreat to offices hermetically sealed against intrusion of clinical collaborations, with computer-based model tuning activities having become "the main challenge when using the CRM" [@lee_calibration_2011].^[This reframing of tuning as 'calibration' is itself notable, and will be discussed below.] Biostatisticians remain to this day intent on problems arising out of CRM calibration practice, as demonstrated in the recent contribution by Braun [-@braun_simulationfree_2020], who develops a mean-field approximation that greatly accelerates CRM performance characterization and calibration. Applied to a previously-completed trial, this approximation technique reduces certain calibration steps from 6.3 hours to 18 seconds (1260$\times$) and from 2 days to 2 minutes ($\approx 1400 \times$).

Here I revisit Braun's application from the perspective of *complete path enumeration* (CPE), which package `precautionary` now implements for the CRM.

# Performance characterization for a given CRM model

Prerequisite to any CRM prior calibration procedure aiming to optimize selected performance characteristics, is the ability to characterize performance for any given CRM model with fixed prior parameters.^[This is just the observation that we must be able to calculate an objective function in order to optimize it.] So we begin by repeating Braun's characterization of his calibrated model.^[This starts at the bottom of page 8 in @braun_simulationfree_2020.] The calibrated model is as follows:

```{r calibrated-model}
skel_0 <- c(0.03, 0.11, 0.25, 0.42, 0.58, 0.71)
calmod <- Crm$new(skeleton = skel_0,
                  scale = 0.85, # aka 'sigma'
                  target = 0.25)
```

Braun's initial characterization is of trials of fixed enrollment $N=30$, with a cohort size of 2.^[The cohort size is not mentioned in the text, but see the example at the bottom of Appendix 1, where Braun sets `cohort_size <- 2`.] Trials of this size readily yield to CPE on desktop hardware:

```{r cpe-bach}
d1_maxn <- 5
cum_maxn <- 10
system.time({
  calmod$
    no_skip_esc(TRUE)$    # compare Braun's 'restrict = T'
    no_skip_deesc(FALSE)$
      stop_func(function(x) {
        enrolled <- tabulate(x$level, nbins = length(x$prior))
        x$stop <- enrolled[1] >= d1_maxn || max(enrolled) >= cum_maxn
        x
      })$trace_paths(root_dose = 1,
                     cohort_sizes = rep(2, 15), # ==> N=30
                     impl = 'rusti') # fast Rust implementations of CRM integrands
  
  T <- calmod$path_array()
})
```

The 3-dimensional array `T[j,c,d]` has the same structure and function as in @norris_what_2020:

```{r inspect-T}
dim(T)
```

Along each of the `r dim(T)[1]` paths enumerated, toxicities have been tabulated separately for as many as `r dim(T)[2]` distinct cohorts enrolled at each of `r dim(T)[3]` doses.

Again as in @norris_what_2020, we may ...

## Compute $\mathbf{b} = \sum_c {n \choose T_{c,d}}$

```{r b-via-T}
b <- apply(log(choose(2, T)), MARGIN = 1, FUN = sum, na.rm = TRUE)
length(b)
```

## Compute $\mathrm{U}$ and $\mathrm{Y}$

```{r Y-soeasy}
Y <- apply(T, MARGIN = c(1,3), FUN = sum, na.rm = TRUE)
Z <- apply(2-T, MARGIN = c(1,3), FUN = sum, na.rm = TRUE)
U <- cbind(Y, Z)
dim(U)
```

## Express $\mathbf{\pi}$ as a function of dose-wise DLT probabilities

```{r pi-function}
log_pi <- function(prob.DLT) {
  log_p <- log(prob.DLT)
  log_q <- log(1 - prob.DLT)
  b + U %*% c(log_p, log_q)
}
```

Note in particular that $\mathbf{\pi}(\mathbf{p})$ is a function of the DLT probabilities at the design's prespecified doses, and so is defined on the *simplex* $0 < p_1 < p_2 < ... < p_D < 1$. To demonstrate that the constraint $\sum_j \pi^j \equiv 1$ holds over this domain, let us check at a randomly chosen point:^[But note this constraint holds identically, simply by the construction of $\mathrm{U}$ from complementary left and right halves, $\mathrm{Y}$ and $\mathrm{Z}$.]

```{r pi-sum-check}
p <- sort(runif(6)) # randomly select a DLT probability vector
sum(exp(log_pi(p))) # check path probabilities sum to 1
```

Together with this ability to compute the path probabilities $\mathbf{\pi}(\mathbf{p})$ under any scenario $\mathbf{p}$, $T$'s complete frequentist characterization of the path outcomes enables us immediately to obtain any of the common performance characteristics of interest:^[I briefly suspend judgment as to whether we *should* do this, and indeed whether these performance characteristics make any sense at all. These questions are examined below.]

## Probability of Correct Selection (PCS)

Whereas the array $T_{c,d}^j$ contains clinical *events* (enrollments, toxicities) along each path, the final dose *recommendations* are retained as the rightmost non-`NA` value in each row of `calmod`'s *path matrix*.^[This matrix retains the columnar layout---although *not* the row degeneracy---of the *dose transition pathways* (DTP) tables of package [`dtpcrm`](https://CRAN.R-project.org/package=dtpcrm)] The `path_rx()` method of the `Crm` class returns this:

```{r recs}
xtabs(~calmod$path_rx())
```

These dose recommendations of course must be weighted by the path probabilities of some chosen dose-toxicity scenario. For sake of an easy demonstration, let's take our scenario from the skeleton itself:

```{r pcs}
options(digits=4)
pi_skel <- calmod$path_probs(calmod$skeleton())
xtabs(pi_skel ~ calmod$path_rx())
```

According to our skeleton, dose 3 is 't
  * he' MTD; so PCS is `r round(xtabs(calmod$path_probs(calmod$skeleton()) ~ calmod$path_rx())[3],2)` in this scenario.

## Fraction Assigned to MTD

The array $T_{c,d}^j$ lets us count patients assigned to 'the' MTD:

```{r fatm}
path_cohorts <- apply(!is.na(T), MARGIN=1, FUN=sum, na.rm=TRUE)
path_MTDcohs <- apply(!is.na(T[,,3]), MARGIN=1, FUN=sum, na.rm=TRUE)
sum(pi_skel * path_MTDcohs / path_cohorts)
```

## Fraction 'Overdosed'

A similar calculation enables us to calculate the fraction assigned to doses exceeding 'the' MTD:

```{r supposedly-overdosed}
path_ODcohs <- apply(!is.na(T[,,4:6]), MARGIN=1, FUN=sum, na.rm=TRUE)
sum(pi_skel * path_ODcohs / path_cohorts)
```

Furthermore, we can ask what fraction of thus-'overdosed' trial participants may be expected to exceed *their own* MTD$_i$'s:^[That is, to experience a DLT. Note that this latter fraction is a *patient-centered* notion of 'overdose', as opposed to the traditionally dose-centered notion.]

```{r actually-overdosed}
path_DLTs4_6 <- apply(T[,,4:6], MARGIN=1, FUN=sum, na.rm=TRUE)
sum(pi_skel * path_DLTs4_6) / sum(pi_skel * 2*path_ODcohs)
```

## Extensions to scenario ensembles

As the examples above illustrate, once a fixed trial design has been completely path-enumerated, its performance characteristics are available through fast array operations. Extending single-scenario performance evaluations to ensemble-average evaluations ought to prove straightforward. A demonstration is not offered here.

## Extending to larger trials

```{r scaly, eval=FALSE}
saved <- options(mc.cores = 6)
kraken <- data.frame(C = 11:25, J = NA_integer_, elapsed = NA_real_)
for (i in 1:nrow(kraken)) {
  C <- kraken$C[i]
  calmod$skeleton(calmod$skeleton()) # reset skeleton to clear cache for honest timing
  time <- system.time(
    calmod$trace_paths(1, rep(2,C), impl='rusti', unroll=8))
  kraken$elapsed[i] <- time['elapsed']
  kraken$J[i] <- dim(calmod$path_matrix())[1]
  print(kraken)
}
options(saved)
```

```{r kraken, echo=FALSE}
kraken <- eval(parse(text = "structure(list(C = c(11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25), J = c(5001, 7839, 11571, 16125, 22041, 28485, 35589, 43371, 48255, 50379, 52011, 53085, 53205, 53205, 53205), elapsed = c(2.061, 2.938, 3.961, 5.05, 7.172, 8.937, 11.022, 13.407, 14.696, 14.912, 15.298, 15.421, 15.477, 15.515, 15.807)), row.names = c(NA, -15L), class = \"data.frame\")"))

kraken %>%
  kable(
    digits=2,
    caption="Size of complete path enumeration (CPE) for the model, enrolling from 11 to 25 cohorts of 2. Note that all paths of the trial terminate by the 23rd cohort. Timings (in seconds) obtained running CPE single-threaded on on a 2.6 GHz Core i7 processor."
  ) %>% kable_styling(full_width=FALSE, position="left")
```

<!--
    C     J elapsed
1  11  5001   2.061
2  12  7839   2.938
3  13 11571   3.961
4  14 16125   5.050
5  15 22041   7.172
6  16 28485   8.937
7  17 35589  11.022
8  18 43371  13.407
9  19 48255  14.696
10 20 50379  14.912
11 21 52011  15.298
12 22 53085  15.421
13 23 53205  15.477
14 24 53205  15.515
15 25 53205  15.807
-->

```{r kraken-plot, fig.cap="Number of paths *J* vs enrollment for the CRM trial at hand. Note that all paths hit stopping criteria no later than at N = 46 (23 cohorts of 2), so that what begins as exponential growth (the linear trend for N < 30 in this semi-log plot) rapidly succumbs to this hard upper bound.", echo=FALSE}
kraken$N <- 2L * kraken$C
oldpar <- par(mar = c(5,6,1,1))
plot(J ~ N, log="y", data=subset(kraken, C<=23), las=1, xlab="", ylab="")
mtext("J", side=2, line=4.0, las=1)
mtext("Enrollment", side=1, line=2.5, las=1)
par(oldpar)
```

Reassuringly, the (perhaps typical) stopping criteria in this trial impose a finite upper bound on CPE size stringent enough to keep the space and time complexity of CPE feasible. Even without such planned stopping rules, as a practical matter, once a dose-finding trial has enrolled more than 30 participants, it ought to have yielded enough information to warrant revisiting the original design. This limits the reasonable 'time between overhauls'^[See https://en.wikipedia.org/wiki/Time_between_overhauls.] for a fixed CRM trial design. Thus, CPE-based performance characterization may prove entirely feasible for most practical applications of the standard CRM model.^[This perhaps does *not* apply to applications such as TITE CRM, where participants are modeled non-exchangeably. CPE for TITE CRM remains an open problem.]


# Calibration of CRM priors

The foregoing analysis shows that CPE-based performance characterization for a *given* CRM model of typical size may require on the order of 15 seconds on quite modest hardware. To *calibrate* a CRM model via CPE, however, may incur this cost repeatedly, during what will generally amount to the derivative-free optimization of a discontinuous objective function over the prior-parameter space. For the example at hand, the Nelder-Mead method finds a PCS-optimal skeleton in just 25 minutes:

```{r pcs-fun}
scenario <- skel_0 # for a demo scenario, we use our original skeleton
pcs <- function(s, unroll=6) { # unroll=6 yields quickest PCS for C=15 case
  if (any(diff(s) <= 0) || s[1] <= 0 || s[length(s)] >= 1)
    return(NA) # out-of-bounds
  calmod$skeleton(s)
  calmod$trace_paths(1, rep(2,15), impl='rusti', unroll=unroll)
  pi_0 <- calmod$path_probs(scenario)
  correct <- calmod$path_rx() == 3
  sum(pi_0[correct])
}
```

```{r nelder-mead, eval=FALSE}
optim(calmod$skeleton(), pcs, method="Nelder-Mead",
      control = list(fnscale = -1, # NEGative ==> MAXimize PCS
                     reltol = 0.001, trace = 2))
```

```
Exiting from Nelder Mead minimizer
    135 function evaluations used
$par
[1] 0.03040 0.08371 0.23867 0.43310 0.59292 0.72230

$value
[1] 0.5931

$counts
function gradient 
     135       NA 

$convergence
[1] 0

$message
NULL
```


## Miscellaneous optimizations

Cheung and Chappell [-@cheung_simple_2002] develop an asymptotic assessment capable of rejecting certain skeletons as *intrinsically* unreasonable because they confer inadequate sensitivity on the CRM. This criterion applies irrespective of dose-toxicity scenario, and so might usefully restrict our skeleton search during calibration by Nelder-Mead or similar direct-search method. The dimensionality of this search might also be addressed directly, by parametrizing the skeleton in 2 or 3 suitable basis dimensions. But, as will become apparent presently, I do not think this type of calibration exercise ought to be taken so seriously.

# On calibrating 'priors'

In everyday usage, *calibration* suggests an exacting procedure, in which typically a measuring instrument is adjusted to accord with some objective standard or criterion. In statistics, however, the term may not carry quite this connotation. Certainly, @grieve_idle_2016 discusses the calibration of Bayesian trial designs generally in much the same sense as our 'CRM calibration' here, with the criterion being provided by whatever frequentist properties are deemed desirable in each given application.^[Interestingly, such activities in relation to the CRM were first discussed by @chevret_continual_1993 using the more prosaic term 'tuning'.]

Whatever we call CRM skeleton-tweaking, the very act of treating the CRM's prior dose-toxicity probabilities as *free parameters* remains problematic for the CRM. To a large degree, undermines the interpretation originally claimed for them demonstrates that they no longer bear the relation to real probabilities originally claimed for them.
This transition in viewing the What then is the objective standard to which One generous way to interpret *calibration* is to reco

TODO: Here, I need to offer as concisely as possible a swift overview of the model-calibration problem, and several prominent approaches to it.

Perhaps start with [@yin_bayesian_2009], as the 'definitive' expression of a sincere Bayesian approach to the calibration problem. Then, introduce the L+S 2009,2011 papers as characteristic of the 'empirical' alternative which Braun has taken up and which shapes the approach I adopt *for purposes of criticism* in this vignette.

However, I might be stuck with a historical approach that goes back to [@chevret_continual_1993]! (Why does this always happen to me?)
Remarkably, [@chevret_continual_1993] exhibits concern for *overall toxicities* as a performance characteristic.

# Aims

@braun_simulationfree_2020 advanced a heuristic for fast, approximate analysis of certain 'performance characteristic' which have traditionally been of interest to the designers of 1-size-fits-all dose-finding trials. With version 0.2-2, package `precautionary` introduces a highly performant algorithm for complete path enumeration of such trials, which enables---by means of a matrix formalism outlined in @norris_what_2020---an exact computation of these same characteristics. The purpose of this vignette is to compare the performance of the latter technique with the former.

# PLAN

Having revisited the Braun 2020 sim-free paper on 4/12/21, with some serious CRM numerics programming under my belt, I now see a fuller shape for this paper. It might work best to deal with the two separate simulation modes of Braun in the opposite order in which he pursues them:

1. Starting at the very bottom of p.8, Braun proceeds from a fixed skeleton and sigma, examining trials enrolling as many as N=100 participants, under 1000 different (random) true DLT probability settings. This (fixed-skeleton, fixed-sigma) mode of analysis is a point of special strength for my 'exact' approach. A single N=100 trial (if this were even feasible to trace all paths for) would yield a single (enormous!) T array which would enable PCS and other performance characteristics to be computed by fast matrix math.

The N=100 of Braun's simulation is however quite challenging! Under the best of circumstances, might it turn out that allowing paths to 'merge' on tallies reduces the size of the matrix to something manageable? I doubt this really could work without some serious pruning of the exponentially-growing sheer *number* of paths, based e.g. on their not rising above some minimum probability threshold. But then again, there might be stopping criteria that quite effectively prune those paths. In any case, exploring that far out into the exponential explosion would require some kind of on-demand extensble computation not unlike my extensible sims.

I may gain more by an argument that emphasizes the practical irrelevance of such large trials. (Why should I feel obliged to play a pointless game?) Plainly, the conceit of retaining the original calibration past the first N~40 or so patients lacks credibility: after 40 patients dosed, any decent dose-finding experiment will have generated enough understanding of the clin pharm to render all pre-trial design decisions obsolete. (But doesn't Bayesianism in theory offer an excellent defense against this complaint? Or does that defense depend on precisely the asymptotic behavior we are trying to characterize?)

2. Braun's first exploration, with fixed 'truth' and flexible skeleton & sigma, is probably infeasible to 'take literally'. So my approach has to be about reducing the dimension of skeleton and sigma, so that fewer than Braun's 1000 (random) skeletons need to be assessed. To this end, I think I can appeal to the pharmacological rationality that CRM (and model-based methods generally) presume for themselves. My point must be that a random search for skeleton is itself a fictional undertaking that serves more to generate a performance benchmark than to solve a real problem in drug development. (These are perhaps the most important points I can make in this vignette!)

3. I probably need a whole section on 'confessions' that the literature effectively makes. One in particular (see [@chevret_continual_1993, p.1100]) is the retention of the same fixed doses out to some large number of patients!

4. It will also be very helpful to lean on [fractional factorial designs](https://en.wikipedia.org/wiki/Fractional_factorial_design), as a means to extend CPE-based trial analysis into the difficult territory Braun 2020 treks through.

5. The whole question of how prior information is (not) used in CRM is fascinating. What does [@grieve_idle_2016] offer in regard to the connection between *calibration* and *prior information*? What does it tell us, that the CRM so rapidly lost its supposed connection with prior elicitation in the early 1990s? Already by 1995 we have [@goodman_practical_1995] starting from the lowest dose, insead of one selected on basis of the skeleton.

## Misc. reading notes

* As I plumb the 2 Lee + Cheung papers, and some of their refs, I appreciate the complexity (and confusion!) of the literature around this.

* The Bayesian model averaging of Yin and Yuan [-@yin_bayesian_2009] can't be ignored in my overall effort to bring sense and coherence to the CRM calibration literature.

```{r echo=FALSE, results='hide'}
options(old) # restore user's original options before finishing, per CRAN
```

```{r bib, include=FALSE, cache=FALSE}
# Create a bib file for packages cited in this paper
knitr::write_bib(c('dtpcrm'), file = 'packages.bib')
```

# References
