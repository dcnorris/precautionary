---
title: "MCSE-Free CRM Performance and Safety Assessment"
author: "David C. Norris"
date: "4/7/2021"
output:
  bookdown::tufte_html2:
    highlight: pygments
vignette: <
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{MCSE-Free CRM Performance and Safety}
  \usepackage[utf8]{inputenc}
bibliography:
  - precautionary-package.bib
  - packages.bib
header-includes:
  \newcommand{\MTDi}{\mathrm{MTD}_i}
  \newcommand{\MTDig}[1][g]{\mathrm{MTD}_i^{#1}}
  \newcommand{\CV}{\mathrm{CV}}

---

```{r setup, include=FALSE}
old <- options(rmarkdown.html_vignette.check_title = FALSE) # suppress un-needed warning
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.height = 4, fig.width = 6)
knitr::opts_knit$set(eval.after = "fig.cap")

library(precautionary)
library(knitr)
library(kableExtra)
##library(dplyr)
##library(dtpcrm)
##library(latticeExtra)

options(ordinalizer = NULL) # vignette presumes this is not already set

# Echo source focused on package functionality by redacting calls to kable()
# see https://bookdown.org/yihui/rmarkdown-cookbook/hook-hide.html:
local({
  hook_source <- knitr::knit_hooks$get('source')
  knitr::knit_hooks$set(source = function(x, options) {
    x <- gsub(" -> etc.", "", x)
    x <- x[!grepl("^etc. %>%", x)] # strip lines starting "etc. %>% ..."
    x <- x[!grepl("\\(etc.,", x)]  # strip lines with fun(etc., ...)
    hook_source(x, options)
  })
})
```

> "In most trials, and in particular in cancer chemotherapy trials, it is rare that we have no idea at all about the dose-response relationship. Such information is implicitly used in the choice of dose levels available for use in the trial, and part of our approach here is to make an attempt to quantify such information."^[@oquigley_continual_1990]

> "It is important to see to what extent the largely arbitrary specification of these features influences the operational nature of the method, as well as the conclusions reached.
> Experimenters prefer methods which are relatively insensitive to prior specification as well as the form of the working model."^[@chevret_continual_1993]

> "Most clinicians do not feel sufficiently confident in their initial toxicity probability estimates to start above the lowest dose (often chosen to be 10% of rodents' LD$_{10}$)."^[@goodman_practical_1995]

> "The main challenge when using the CRM is model calibration."^[@lee_calibration_2011]


# Background

Since its inception 3 decades ago [@oquigley_continual_1990], the CRM has retreated utterly from the sincere Bayesianism of its original claim to manifest clinically relevant prior information. The early retreat was quite swift, in fact. Section 2.4 of [@oquigley_continual_1990], titled 'Establishing Dose Levels', suggested a substantal collaboration between modelers and clinical investigators. Yet already by 1993, [@chevret_continual_1993] accepted "that $k$ distinct doses are chosen for experimentation, defined by the investigator through an implicit idea of the dose-toxicity relationship, however imprecise," and demoted the CRM's prior dose-toxicity curve to the status of a (multidimensional) *tuning parameter*. The retreat continued apace with @goodman_practical_1995 unmistakably revealing the deep skepticism with which clinical investigators were already viewing a CRM that failed to capture important intutions, abandoning any role for this prior curve in informing the trial starting dose.

By twenty years later, biostatisticians' had completed their retreat to offices hermetically sealed against intrusion of clinical collaborations, with computer-based model tuning activities having become "the main challenge when using the CRM" [@lee_calibration_2011].^[This reframing of tuning as 'calibration' is itself notable, and will be discussed below.] Biostatisticians remain to this day intent on problems arising out of CRM calibration practice, as demonstrated in the recent contribution by Braun [-@braun_simulationfree_2020], who develops a mean-field approximation that greatly accelerates CRM performance characterization and calibration. Applied to a previously-completed trial, this approximation technique reduces certain calibration steps from 6.3 hours to 18 seconds (1260$\times$) and from 2 days to 2 minutes ($\approx 1400 \times$).

Here I revisit Braun's application from the perspective of *complete path enumeration* (CPE), which package `precautionary` now implements for the CRM. Because CPE in order to place this emergent preoccupation with calibration in a fresh context.

# Performance characterization for a given CRM model

Prerequisite to any CRM prior calibration procedure aiming to optimize selected performance characteristics, is the ability to characterize performance for any given CRM model with fixed prior parameters.^[This is just the observation that we must be able to calculate an objective function in order to optimize it.] So we begin by repeating Braun's characterization of his calibrated model.^[This starts at the bottom of page 8 in @braun_simulationfree_2020.] The calibrated model is as follows:

```{r calibrated-model}
calmod <- Crm$new(skeleton = c(0.03, 0.11, 0.25, 0.42, 0.58, 0.71),
                  scale = 0.85, # aka 'sigma'
                  target = 0.25)
```

Braun's initial characterization is of trials of fixed enrollment $N=30$, with a cohort size of 2.^[The cohort size is not mentioned in the text, but see the example at the bottom of Appendix 1, where Braun sets `cohort_size <- 2`.] Trials of this size readily yield to CPE on desktop hardware:

```{r cpe-bach}
d1_maxn <- 5
cum_maxn <- 10
system.time({
  calmod$
    no_skip_esc(TRUE)$    # compare Braun's 'restrict = T'
    no_skip_deesc(FALSE)$
      stop_func(function(x) {
        enrolled <- tabulate(x$level, nbins = length(x$prior))
        x$stop <- enrolled[1] >= d1_maxn || max(enrolled) >= cum_maxn
        x
      })$trace_paths(root_dose = 1,
                     cohort_sizes = rep(2, 15), # ==> N=30
                     impl = 'rusti') # fast Rust implementations of CRM integrands
  
  T <- calmod$path_array()
})
```

The 3-dimensional array `T[j,c,d]` has the same structure and function as in @norris_what_2020:

```{r inspect-T}
dim(T)
```

Along each of `r dim(T)[1]` the paths enumerated, toxicities have been tabulated separately for as many as `r dim(T)[2]` distinct cohorts enrolled at each of `r dim(T)[3]` doses.

Again as in @norris_what_2020, we may ...

## Compute $\mathbf{b} = \sum_c {n \choose T_{c,d}}$

```{r b-via-T}
b <- apply(log(choose(2, T)), MARGIN = 1, FUN = sum, na.rm = TRUE)
length(b)
```

## Compute $\mathrm{U}$ and $\mathrm{Y}$

```{r Y-soeasy}
Y <- apply(T, MARGIN = c(1,3), FUN = sum, na.rm = TRUE)
Z <- apply(2-T, MARGIN = c(1,3), FUN = sum, na.rm = TRUE)
U <- cbind(Y, Z)
dim(U)
```

## Express $\mathbf{\pi}$ as a function of dose-wise DLT probabilities

```{r pi-function}
log_pi <- function(prob.DLT) {
  log_p <- log(prob.DLT)
  log_q <- log(1 - prob.DLT)
  b + U %*% c(log_p, log_q)
}
```

Note in particular that $\mathbf{\pi}(\mathbf{p})$ is a function of the DLT probabilities at the design's prespecified doses, and so is defined on the *simplex* $0 < p_1 < p_2 < ... < p_D < 1$. To demonstrate that the constraint $\sum_j \pi^j \equiv 1$ holds over this domain, we exhibit the fact at a randomly chosen point:

```{r pi-sum-check}
p <- sort(runif(6)) # randomly select a DLT probability vector
sum(exp(log_pi(p))) # check path probabilities sum to 1
```

Together with this ability to compute the path probabilities $\mathbf{\pi}(\mathbf{p})$ under any scenario $\mathbf{p}$, $T$'s complete frequentist characterization of the path outcomes enables us immediately to obtain any of the common performance characteristics of interest:^[I briefly suspend judgment as to whether we *should* do this, and indeed whether these performance characteristics make any sense at all. These questions are examined below.]

## Probability of Correct Selection (PCS)

Whereas the array $T_{c,d}^j$ contains clinical *events* (enrollments, toxicities) along each path, the final dose *recommendations* are retained as the rightmost non-`NA` value in each row of `calmod`'s 'path matrix':^[This matrix retains the columnar layout---although *not* the row degeneracy---of the *dose transition pathways* (DTP) tables of package [`dtpcrm`](https://CRAN.R-project.org/package=dtpcrm)]

```{r recs}
pmx <- calmod$path_matrix()
## Obtain path-wise index into final dose recs
rxcol <- max.col(!is.na(pmx), ties.method="last")
rxdose <- pmx[cbind(seq.int(nrow(pmx)),rxcol)]
xtabs(~rxdose)
```

These dose recommendations of course must be weighted by the path probabilities of some chosen dose-toxicity scenario. For sake of an easy demonstration, let's take our scenario from the skeleton itself:

```{r pcs}
pi_skel <- exp(log_pi(prob.DLT = calmod$skeleton()))
options(digits=4)
xtabs(pi_skel ~ rxdose)
```

According to our skeleton, dose 3 is 'the' MTD; so PCS is `r round(xtabs(pi_skel ~ rxdose)[3],2)` in this scenario.

## Fraction Assigned to MTD

The array $T_{c,d}^j$ lets us count patients assigned to 'the' MTD:

```{r fatm}
path_cohorts <- apply(!is.na(T), MARGIN=1, FUN=sum, na.rm=TRUE)
path_MTDcohs <- apply(!is.na(T[,,3]), MARGIN=1, FUN=sum, na.rm=TRUE)
sum(pi_skel * path_MTDcohs / path_cohorts)
```

## Fraction 'Overdosed'

A similar calculation enables us to calculate the fraction assigned to doses exceeding 'the' MTD:

```{r supposedly-overdosed}
path_ODcohs <- apply(!is.na(T[,,4:6]), MARGIN=1, FUN=sum, na.rm=TRUE)
sum(pi_skel * path_ODcohs / path_cohorts)
```

Furthermore, we can ask what fraction of thus-'overdosed' trial participants may be expected to exceed *their own* MTD$_i$'s:^[That is, to experience a DLT.]

```{r actually-overdosed}
path_DLTs4_6 <- apply(T[,,4:6], MARGIN=1, FUN=sum, na.rm=TRUE)
sum(pi_skel * path_DLTs4_6) / sum(pi_skel * 2*path_ODcohs)
```

## Extensions to scenario ensembles

As the examples above illustrate, once a fixed trial design has been completely path-enumerated, its performance characteristics are available through fast array operations. Extending single-scenario performance evaluations to ensemble-average evaluations is therefore straightforward and fast.

## Extending to larger trials

```{r scaly, eval=FALSE}
skel <- calmod$skeleton()
kraken <- data.frame(C = 11:25, J = NA_integer_, user = NA_real_, elapsed = NA_real_)
for (i in 1:nrow(kraken)) {
  C <- kraken$C[i]
  calmod$skeleton(skel) # set skeleton to erase cache for honest timing
  time <- system.time(
    calmod$trace_paths(1, rep(2,C), impl='rusti'))
  kraken$user[i] <- time['user.self'] + time['user.child']
  kraken$elapsed[i] <- time['elapsed']
  kraken$J[i] <- dim(calmod$path_matrix())[1]
  print(kraken)
}
```

```{r kraken, echo=FALSE}
kraken <- eval(parse(text = "structure(list(C = 11:25, J = c(5001L, 7839L, 11571L, 16125L, 22041L, 28485L, 35589L, 43371L, 48255L, 50379L, 52011L, 53085L, 53205L, 53205L, 53205L), user = c(3.705, 6.42, 10.26, 15.89, 24.295, 34.552, 50.251, 69.854, 85.403, 93.245, 99.054, 103.047, 104.059, 104.409, 99.604), elapsed = c(3.76, 6.568, 10.595, 16.526, 25.434, 36.42, 53.137, 73.998, 90.52, 98.86, 105.1, 109.306, 110.345, 110.725, 105.684)), row.names = c(NA, -15L), class = \"data.frame\")"))

kraken %>%
  kable(
    digits=2,
    caption="Size of complete path enumeration (CPE) for the model, enrolling from 11 to 25 cohorts of 2. Note that all paths of the trial terminate by the 23rd cohort. Timings (in seconds) obtained running CPE single-threaded on on a 2.6 GHz Core i7 processor."
  ) %>% kable_styling(full_width=FALSE, position="left")
```

<!--
    C     J    user elapsed
1  11  5001   3.705   3.760
2  12  7839   6.420   6.568
3  13 11571  10.260  10.595
4  14 16125  15.890  16.526
5  15 22041  24.295  25.434
6  16 28485  34.552  36.420
7  17 35589  50.251  53.137
8  18 43371  69.854  73.998
9  19 48255  85.403  90.520
10 20 50379  93.245  98.860
11 21 52011  99.054 105.100
12 22 53085 103.047 109.306
13 23 53205 104.059 110.345
14 24 53205 104.409 110.725
15 25 53205  99.604 105.684
-->

```{r kraken-plot, fig.cap="Number of paths *J* vs enrollment for the CRM trial at hand. Note that all paths hit stopping criteria no later than at N = 46 (23 cohorts of 2), so that what begins as exponential growth (the linear trend for N < 30 in this semi-log plot) ultimately succumbs to this hard upper bound.", echo=FALSE}
kraken$N <- 2L * kraken$C
oldpar <- par(mar = c(5,6,1,1))
plot(J ~ N, log="y", data=subset(kraken, C<=23), las=1, xlab="", ylab="")
mtext("J", side=2, line=4.0, las=1)
mtext("Enrollment", side=1, line=2.5, las=1)
par(oldpar)
```

Reassuringly, the (perhaps typical) stopping criteria in this trial impose a finite upper bound on CPE size stringent enough to keep the space and time complexity of CPE feasible. Even without such planned stopping rules, as a practical matter, once a dose-finding trial has enrolled more than 30 participants, it ought to have yielded enough information to warrant revisiting the original design. This limits the reasonable 'time between overhauls'^[See https://en.wikipedia.org/wiki/Time_between_overhauls.] for a fixed CRM trial design. Thus, CPE-based performance characterization may prove entirely feasible for most practical applications of the standard CRM model.^[This perhaps does *not* apply to applications such as TITE CRM, where participants are modeled non-exchangeably. CPE for TITE CRM remains an open problem.]


# Calibration of CRM priors

The foregoing analysis shows that single-threaded CPE-based performance characterization for a *given* CRM model of typical size may require on the order of 1 minute of single-threaded computation. To *calibrate* a CRM model via CPE, however, may incur this cost repeatedly, during what in general involves derivative-free optimization of a discontinuous objective function over the prior-parameter space. Whereas the computation of gradients and higher derivatives lends itself trivially to parallelization, the operations of the Nelder-Mead method are mainly sequential, and inherently parallel pattern-search^[https://en.wikipedia.org/wiki/Pattern_search_(optimization)] alternatives are not readily available in R. My sense of particle-swarm and simulated annealing methods is that they may trade away more in robustness than they could gain by parallelization in this application.

such as PatternMore parallelizable Other derivative-free methods Intrinsically parallel methods such as  Intrinsically parallelizable methods The dimensionality of this space will typically be on par with the number of cores on modern CPUs,^[A standard 1-parameter CRM model with $D$ pre-specified doses will have a length-$D$ skeleton vector plus a prior variance parameter $\sigma$.] and typical optimization routines will perform a comparable number of objective evaluations per step. Suitably parallelized, the optimization itself may therefore require about 1 minute per step.


For example, applying Within-step parallelization thus promises Thus these costs may be parallelized An optimization routine that performsIf the number of evaluations per optimizer step is comparable to this dimension, then This incurs the cost of CPE repeatedly, although multipl pr tp valuations may well be fully parallelizable across.

For at each point sampled. Generally, however, these costs will be substantially parallelizable.

Calibrating the skeleton of a CRM design with $D$ doses requires optimizing a non-differentiable function on a $D$-dimensional simplex. The Nelder-Mead method would run $D+1$ CPE computations per step, which for typical CRM trials could be run in parallel on the 8+ independent threads on a modern workstation CPU. Thus each Nelder-Mead step would take on the order of 1 minute, and a dozen steps could run an a quarter-hour. Furthermore, the possibility of running 'scout' calibrations on smaller trials could further economize on computation time.

## Indifference Intervals

Cheung and Chappell [-@cheung_simple_2002] develop an asymptotic assessment capable of rejecting certain skeletons as *intrinsically* unreasonable because they confer inadequate sensitivity on the CRM. This criterion applies irrespective of dose-toxicity scenario, and so confines our skeleton search during calibration to a subset of the aforementioned simplex.


enabling derivation of $\mathbf{b}$ and such that logically precedes any calibration procedure that searches the prior-parameter space for optimal values. After model calibration, Although multi-scenario performance characterization

* Tune the CRM prior to optimize certain frequentist performance characteristics obtained via simulation under a single, fixed dose-toxicity scenario.
* Given a fixed prior, obtain the operating characteristics of the model over a range of simulated dose-toxicity scenarios.

(what is nowadays called the CRM 'skeleton') prior probability
Because the second of Braun's 2 calibrations proves to be so straightforward from the CPE point of view, we will deal with that one first.

the second is extremely straightforward from 

# On calibrating 'priors'

It is interesting to see that the confessional term 'tuning' employed in [@chevret_continual_1993] becomes 'calibration' in later work. In everyday usage, *calibration* involves adjusting typically a measuring instrument so that it accords with some objective standard or criterion. @grieve_idle_2016 however discusses the calibration of Bayesian trial designs in much the same sense, with the criterion being provided by whatever frequentist properties are deemed desirable in each given application.

If we do call these modern tuning efforts 'calibration', then the very act of treating the CRM's prior dose-toxicity probabilities as free parameters undermines the interpretation originally claimed for them demonstrates that they no longer bear the relation to real probabilities originally claimed for them.
This transition in viewing the What then is the objective standard to which One generous way to interpret *calibration* is to reco

TODO: Here, I need to offer as concisely as possible a swift overview of the model-calibration problem, and several prominent approaches to it.

Perhaps start with [@yin_bayesian_2009], as the 'definitive' expression of a sincere Bayesian approach to the calibration problem. Then, introduce the L+S 2009,2011 papers as characteristic of the 'empirical' alternative which Braun has taken up and which shapes the approach I adopt *for purposes of criticism* in this vignette.

However, I might be stuck with a historical approach that goes back to [@chevret_continual_1993]! (Why does this always happen to me?)
Remarkably, [@chevret_continual_1993] exhibits concern for *overall toxicities* as a performance characteristic.

# Aims

TODO: I may have reached a point with all this, such that I am no longer mounting a focused critique of Braun 2020, but rather using it as the basis for a more comprehensive review of prior calibrations approaches and entry-point into my own emphasis on comprehensive path enumeration as the way forward. The general approach might be to offer as coherent and general a framing of the CRM calibration problem as possible, but then use Braun 2020 as the 'challenge' w.r.t. efficiency and speed.

@braun_simulationfree_2020 advanced a heuristic for fast, approximate analysis of certain 'performance characteristic' which have traditionally been of interest to the designers of 1-size-fits-all dose-finding trials. With version 0.2-2, package `precautionary` introduces a highly performant algorithm for complete path enumeration of such trials, which enables---by means of a matrix formalism outlined in @norris_what_2020---an exact computation of these same characteristics. The purpose of this vignette is to compare the performance of the latter technique with the former.

# PLAN

Having revisited the Braun 2020 sim-free paper on 4/12/21, with some serious CRM numerics programming under my belt, I now see a fuller shape for this paper. It might work best to deal with the two separate simulation modes of Braun in the opposite order in which he pursues them:

1. Starting at the very bottom of p.8, Braun proceeds from a fixed skeleton and sigma, examining trials enrolling as many as N=100 participants, under 1000 different (random) true DLT probability settings. This (fixed-skeleton, fixed-sigma) mode of analysis is a point of special strength for my 'exact' approach. A single N=100 trial (if this were even feasible to trace all paths for) would yield a single (enormous!) T array which would enable PCS and other performance characteristics to be computed by fast matrix math.

The N=100 of Braun's simulation is however quite challenging! Under the best of circumstances, might it turn out that allowing paths to 'merge' on tallies reduces the size of the matrix to something manageable? I doubt this really could work without some serious pruning of the exponentially-growing sheer *number* of paths, based e.g. on their not rising above some minimum probability threshold. But then again, there might be stopping criteria that quite effectively prune those paths. In any case, exploring that far out into the exponential explosion would require some kind of on-demand extensble computation not unlike my extensible sims.

I may gain more by an argument that emphasizes the practical irrelevance of such large trials. (Why should I feel obliged to play a pointless game?) Plainly, the conceit of retaining the original calibration past the first N~40 or so patients lacks credibility: after 40 patients dosed, any decent dose-finding experiment will have generated enough understanding of the clin pharm to render all pre-trial design decisions obsolete. (But doesn't Bayesianism in theory offer an excellent defense against this complaint? Or does that defense depend on precisely the asymptotic behavior we are trying to characterize?)

2. Braun's first exploration, with fixed 'truth' and flexible skeleton & sigma, is probably infeasible to 'take literally'. So my approach has to be about reducing the dimension of skeleton and sigma, so that fewer than Braun's 1000 (random) skeletons need to be assessed. To this end, I think I can appeal to the pharmacological rationality that CRM (and model-based methods generally) presume for themselves. My point must be that a random search for skeleton is itself a fictional undertaking that serves more to generate a performance benchmark than to solve a real problem in drug development. (These are perhaps the most important points I can make in this vignette!)

3. I probably need a whole section on 'confessions' that the literature effectively makes. One in particular (see [@chevret_continual_1993, p.1100]) is the retention of the same fixed doses out to some large number of patients!

4. It will also be very helpful to lean on [fractional factorial designs](https://en.wikipedia.org/wiki/Fractional_factorial_design), as a means to extend CPE-based trial analysis into the difficult territory Braun 2020 treks through.

5. The whole question of how prior information is (not) used in CRM is fascinating. What does [@grieve_idle_2016] offer in regard to the connection between *calibration* and *prior information*? What does it tell us, that the CRM so rapidly lost its supposed connection with prior elicitation in the early 1990s? Already by 1995 we have [@goodman_practical_1995] starting from the lowest dose, insead of one selected on basis of the skeleton.

## Misc. reading notes

* As I plumb the 2 Lee + Cheung papers, and some of their refs, I appreciate the complexity (and confusion!) of the literature around this.

* The Bayesian model averaging of Yin and Yuan [-@yin_bayesian_2009] can't be ignored in my overall effort to bring sense and coherence to the CRM calibration literature.

# The case study of Braun (2020)

@braun_simulationfree_2020 reports 2 distinct simulation-based exercises corresponding to model optimization activities which are apparently typical. In the first, he fixes the CRM skeleton for a 6-dos trial, and examines the effect of the $\sigma$ parameter on *probability of correct selection* (PCS). Using the machinery of package `precautionary`, we might do the same as follows:

```{r sigma-effect, eval=FALSE}

stop_func <- function(x) {
    y <- stop_for_excess_toxicity_empiric(x,
                                          tox_lim = target.DLT + 0.1,
                                          prob_cert = 0.72,
                                          dose = 1)
    if(y$stop){
      x <- y
    } else {
      x <- stop_for_consensus_reached(x, req_at_mtd = 12)
    }
  }


stopping_rule <- function(x, d1_maxn, cum_maxn) {
  enrolled <- tabulate(x$level, nbins = length(x$prior))
  x$stop <- enrolled[1] >= d1_maxn || max(enrolled) >= cum_maxn
  return(x)
}

pcs <- function(sigma, # NB: not vectorized (expects scalar sigma)
                skeleton = c(0.01, 0.03, 0.11, 0.25, 0.41, 0.57),
                target.toxrate = 0.25,
                cohort.size = 2,
                d1_maxn = 5,
                cum_maxn = 10) {
  ## 1. Enumerate the full DTP
  craken <- Crm$new(skeleton = skeleton,
                    scale = sigma,
                    target = target.toxrate)$
    stop_func(function(x) {
      enrolled <- tabulate(x$level, nbins = length(x$prior))
      x$stop <- enrolled[1] >= d1_maxn || max(enrolled) >= cum_maxn
      x
    })
  ## $
  ##   no_skip_esc(TRUE)$             # TODO: Figure out (or ask)
  ##     no_skip_deesc(FALSE)$        #       which escalation rules
  ##       global_coherent_esc(FALSE) #       Braun applied.

  paths <- craken$paths(root_dose = 1,
                        cohort_sizes = rep(cohort.size, 15),
                        impl = 'rusti')

  as.matrix(unique(dtps))
}

## Interestingly, the 3^15 = 14,348,907 ex ante paths
## yield just 35,817 distinct paths with DTP degeneracy.
##
## TODO: Find out how many distinct TALLIES there are on these paths.
## TODO: Figure out whether any condensation of the J dimension (# paths)
##       is possible. Perhaps only one coefficient per possible value of
##       ((n+1) choose T) needs to be accumulated?
## Do check Eq (4) from WWTT on this. Perhaps I can left-multiply by some
## Y*J matrix that condenses the J dimension to an L (taLLy) dim?
## Note that the *point* of all this could be to show matters are not even
## as alarming as WWTT Table 1 would suggest.

##   ## 2. Construct the DTP matrix and T[,,] array
##   path_matrix <- as.matrix(unique(dtps))
##   I <- outer(path_matrix[,paste0("D",0:14)], 1:15, FUN = "==")
##   I[I] <- 1
##   I[!I] <- NA
##   ## Now I[j,c,d] is an indicator array that we may use
##   ## in the manner of a bitmask, to select the toxicities
##   ## into their proper positions within T[j,c,d]:
##   T <- I * outer(path_matrix[,paste0("T",1:15)], rep(1,15))
##   dimnames(T)[[2]] <- paste0("C",1:15) # best labeled as Cohorts
##   dimnames(T)[[3]] <- paste0("X",1:6) # label as doses X1..X7
##   dim(T)

## }

sigmas <- seq(0.7, 2.1, 0.01)

## TODO: Consider doing a mclapply here,
##       keeping the DTP single-threaded.
##       This would most efficiently
##       employ the Crm cache.
t0 <- system.time()
pcss <- sapply(sigmas, pcs)
Dt <- system.time() - t0

plot(pcss ~ sigmas)

```

Braun was able to perform this search in 18 seconds on a MacBook Pro. as given, and explores the consequences of Dose-escalation trial designs commonly operate with a fixed set of $D$ prespecified doses, enrolling participants in small cohorts each of size $n$, at doses selected sequentially according to observed counts of binary dose-limiting toxicities (DLTs). Under such designs, the observations at any point in the trial may be recorded in a $C \times D$ matrix of toxicity counts $\{0, 1, ..., n, -\}$, with '$-$' being used where a cohort has not been enrolled. Indeed, for deterministic designs in which dose-escalation decisions depend strictly on the history of observed DLT counts, such matrices suffice to account for the full dose-escalation sequence.^[To see this, consider that one may 'read' such a matrix, starting from the (deterministic) starting dose, and applying the (deterministic) design rules at each step. That is, a deterministic design *by definition* imposes a unique sequence on the entries in such a matrix.] For designs with upper bounds on enrollment, $C$ may be fixed *ex ante*, and all possible paths comprehensively enumerated.

In @norris_what_2020, for example, paths through the 3+3 design could be represented as $2 \times D$ matrices because the 3+3 design enrolls at most 2 cohorts at any given dose. The mathematical treatment offered there for the path matrices $T_{c,d}^j$ was independent of the 3+3 rules, however, and carries forward generally:

Denoting the prespecified doses by $(X_d)$ and the cumulative distribution function of $\MTDi$ by $P$, we can write the vector $(\pi^j)$ of path probabilities:^[Products or sums over $c$ or pairs $(c,d)$ are understood to be taken over the *non-empty* cohorts thus indexed. In R, this convention is easily applied by using `NA` to represent '$-$', and performing aggregate operations with option `na.rm=TRUE`.]

$$
\begin{align}
p_d &= P(\MTDi < X_d)\quad\mbox{(dose-wise toxicity probabilities)}\\
q_d &= 1 - p_d\\
\pi^j &= \prod_{c,d} {n \choose T_{c,d}^j} p_d^{T_{c,d}^j} q_d^{(n-T_{c,d}^j)}\quad\mbox{(path probabilities)} (\#eq:pi)
\end{align}
$$

Again as in @norris_what_2020, taking logs in \@ref(eq:pi) we find:

$$
\log \boldsymbol{\pi} = \sum_{c,d} \log {n \choose T_{c,d}} + \sum_c [T_{c,d},n-T_{c,d}]\left[{ \log \mathbf{p} \atop \log \mathbf{q}} \right] = \mathbf{b} + U\left[{ \log \mathbf{p} \atop \log \mathbf{q}} \right], (\#eq:logpi)
$$
where the $J \times 2D$ matrix $U$ and the $J$-vector $\mathbf{b}$ are characteristic *constants* of the given dose-escalation design.

Finally, we may as previously introduce the log-therapeutic index $\kappa$, enabling us to write the fatal (grade-5) fraction $f_d$ of DLTs as:

$$
f_d = \frac{P(e^{2\kappa} \MTDi <  X_d)}{P(\MTDi < X_d)}, (\#eq:fatalfraction)
$$

in terms of which the expected number of fatal toxicities is:

$$
\boldsymbol{\pi}^\intercal \mathrm{Y} \mathbf{f},  (\#eq:fatalities)
$$

where $Y = \sum_c T_{c,d}^j$ denotes the $(J \times D)$ left half of $U$.

# Connection with DTP

The DTP idea apparently was introduced primarily as a means to reconcile model-based formulations of dose-escalation trials with the habitually rule-based thinking of clinicians. Specifically, @yap_dose_2017 cites 3 specific "perceived challenges" in making the transition from rule-based to model-based designs:

1. The "flexibility" of model-based designs (which I take to mean the additional free parameters they introduce into the design space) creates choices which cannot readily be appreciated in the rule-based terms familiar to clinical investigators.
2. Benefits of model-based designs are likewise not apparent to clinical investigators, against the background of "rule-based designs perceived to be 'successful' for decades".
3. From the trialist's perspective, the 'black-box' recommendations of model-based designs "contrast unfavorably with the transparent, simple rules of a rule-based design."

To meet these challenges, DTP effectively transforms a model-based design into a rule-based design *locally* --- so that its immediate operation several steps ahead can be 'eyeballed' by trialists as a small set of (e.g., $4^4 = 64$) distinct paths. The exhaustive enumeration employed here as in @norris_what_2020 differs only in its *global* reach, and its intent to support *computation* over the paths as opposed to cursory inspection.

# An application

We will adopt the same parameters as in the `dtpcrm` [package vignette](https://cran.r-project.org/package=dtpcrm/vignettes/dtpcrm_vignettev02.html):^[See Table 1 in @craddock_combination_2019.]

## VIOLA set-up

### Clinical parameters

```{r clinical-params}
number.doses <- 7
start.dose.level <- 3
max.sample.size <- 21
target.DLT <- 0.2
cohort.size <- 3
```

Note that the sample size limit allows us to set $C=7$.

### Model specification parameters

```{r model-spec-params}
prior.DLT <- c(0.03, 0.07, 0.12, 0.20, 0.30, 0.40, 0.52)
prior.var <- 0.75
```

### Early stopping

```{r early-stopping}
stop_func <- function(x) {
  y <- stop_for_excess_toxicity_empiric(x,
                                        tox_lim = target.DLT + 0.1,
                                        prob_cert = 0.72,
                                        dose = 1)
  if(y$stop){
    x <- y
  } else {
    x <- stop_for_consensus_reached(x, req_at_mtd = 12)
  }
}
```

## Computing DTPs

```{r compute-dtp}
t0 <- proc.time()
crm <- Crm$new(skeleton = prior.DLT,
               scale = sqrt(prior.var),
               target = target.DLT)$
  stop_func(stop_func)$
  no_skip_esc(TRUE)$
    no_skip_deesc(FALSE)$
      global_coherent_esc(TRUE)

viola_dtp <- calculate_dtps(
  next_dose = start.dose.level,
  cohort_sizes = rep(3, 7),
  dose_func = crm$applied,
  impl = 'rusti')

proc.time() - t0 # used to take ~17 minutes on a 2.6GHz Quad Core i7
```

```{r load-cached-dtp, echo=FALSE}
data(viola_dtp) # Read cached viola_dtp from disk
```

With each of (up to) 7 cohorts having 4 possible outcomes (0, 1, 2 or 3 toxicities), the DTP tabulation lists a total of $4^7 = 16384$ paths. These are not all distinct paths, however, since early stopping results in path degeneracy.^[Indeed, the underlying calculations exhbit an even more re remarkable degree of degeneracy, which is exploited through memoization implemented in R6 class `Crm` introduced in version 0.2-2 of `precautionary`.] For example, we can see that paths 1005--1008 all terminate at the 6th cohort, resulting in a $4\times$ degeneracy:
```{r degenerates}
knitr::kable(viola_dtp[1000:1010,])
```

Paths terminating at the 5th cohort would be listed 16 times, and those terminating at the 4th cohort 64 times, etc. The net degeneracy indeed proves to be quite substantial, inflating the DTP table by a factor of ```r round(nrow(viola_dtp)/nrow(unique(viola_dtp)), 1)```:
```{r degeneracy}
viola_paths <- as.matrix(unique(viola_dtp))
nrow(viola_paths)
```

```{r echo=FALSE, results='hide'}
options(old) # restore user's original options before finishing, per CRAN
```

```{r bib, include=FALSE, cache=FALSE}
# Create a bib file for packages cited in this paper
knitr::write_bib(c('dtpcrm'), file = 'packages.bib')
```

# References
